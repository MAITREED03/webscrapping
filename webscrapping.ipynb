{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28ee398-642b-4427-b6e2-c491812e271d",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "\n",
    "Web scraping is the process of collecting structured web data in an automated manner1. People primarily use web scraping for obtaining and organizing data for various purposes2. Some of the areas where web scraping is used to get data are123:\n",
    "•\tPrice monitoring and intelligence\n",
    "•\tNews monitoring and analysis\n",
    "•\tReal estate data and market trends\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25979420-4923-4db8-b538-d2fab25b4e23",
   "metadata": {},
   "source": [
    " What are the different methods used for Web Scraping?\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    Web scraping is an automated method used to extract large amounts of data from websites. The data on the websites are unstructured, and web scraping enables us to convert that data into a structured form. There are different methods for web scraping:\n",
    "- Using APIs: Many websites like Google, Twitter, Facebook, StackOverflow, etc. have APIs that allow you to access their data in a structured format.\n",
    "- Creating your own code for web scraping: You can write a code for web scraping from scratch. This is the best option for websites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced.\n",
    "- Using web scraping tools: There are many tools and technologies available for web scraping. Most programming languages like Python or R have specific libraries or extensions that allow you to extract data efficiently. Beautiful Soup, Scrapy, Selenium, and Octoparse are the most widespread web scrapers.\n",
    "- DOM parsing: With web scraping techniques, like DOM parsing, you can extract the exact data you need from a website’s HTML code.\n",
    "- Regular expressions: This is a powerful (albeit complex) way to extract information from text. By creating a regex pattern specific to your scraping needs, you can pull out all matching instances from the text.\n",
    "- XPath: XPath can be used to navigate through elements and attributes in an XML document.\n",
    "Each of these methods has its own advantages and disadvantages and can be used based on your needs. Remember to always respect the terms of service of the website from which you are scraping.\n",
    "\n",
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Beautiful Soup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner.\n",
    "Here are some reasons why Beautiful Soup is used:\n",
    "- Web Scraping: Beautiful Soup helps you pull particular content from a webpage, remove the HTML markup, and save the information. It is a tool for web scraping that helps you clean up and parse the documents you have pulled down from the web.\n",
    "- Data Extraction: As a Python library used for pulling data from HTML, XML, and other markup language files, Beautiful Soup can extract articles and content and turn it into a Python list or dictionary.\n",
    "- Handling Messy Data: Beautiful Soup parses the unwanted data and helps to organize and format the messy web data by fixing bad HTML and present to us in an easily-traversable XML structures.\n",
    "In short, Beautiful Soup is a powerful tool that makes it easy to scrape information from web pages, which is especially useful when you need to gather data for research, development, and analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4.Why is flask used in this Web Scraping project?\n",
    "\n",
    "\n",
    "\n",
    "Flask is often used in web scraping projects for several reasons:\n",
    "- Lightweight: Flask is a lightweight web framework. This makes it a good choice for projects where you want to keep things simple and avoid unnecessary complexity.\n",
    "- Ease of Use: Flask's framework is more explicit than some other frameworks and is also easier to learn. This can make it a good choice for beginners or for projects where you want to get up and running quickly.\n",
    "- Data Presentation: After scraping the web, you often want to present the data you've collected. Flask can be used to parse the collected data and display it as HTML in a new HTML file. This can be particularly useful for web scraping projects where you want to present the results in a user-friendly format.\n",
    "- Integration with Python Libraries: Flask integrates well with Python libraries commonly used in web scraping, such as Beautiful Soup and Requests. This allows you to build a web scraper and a web application to display the scraped data all in one go.\n",
    "- Building Web Applications: In addition to displaying data, Flask can also be used to build web applications. For example, you could build a web application that allows users to interact with your web scraper.\n",
    "In summary, Flask provides a simple and efficient way to both scrape data from the web and present it in a user-friendly format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5.Write the names of AWS services used in this project. Also, explain the use of each service\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "here are some of the AWS services commonly used in a web scraping project and their uses:\n",
    "- AWS Lambda: This is a serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you. In the context of a web scraping project, it can be used to perform stateless tasks.\n",
    "- AWS Fargate: This is a serverless compute engine for containers. It works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. It can be used to execute longer-running, more resource-intense tasks.\n",
    "- AWS DynamoDB: This is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-active, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. In a web scraping project, it can be used as a schema-less data store to manage subscriptions and website states.\n",
    "- AWS CodeBuild: This is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. It can be used in a web scraping project to build and test the code.\n",
    "- AWS CodeCommit: This is a fully managed source control service that hosts secure Git-based repositories. It can be used in a web scraping project for version control of the code.\n",
    "- AWS Batch: This enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. It can be used in a web scraping project to manage batch jobs.\n",
    "- Amazon Elastic Container Registry (ECR): This is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. It can be used in a web scraping project to store and manage Docker images.\n",
    "- Amazon Elastic Container Service (ECS): This is a highly scalable, high-performance container orchestration service that supports Docker containers and allows you to easily run and scale containerized applications on AWS. It can be used in a web scraping project to manage Docker containers.\n",
    "- EventBridge: This is a serverless event bus that makes it easy to connect applications together using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. It can be used in a web scraping project to handle events.\n",
    "- Identity & Access Management (IAM): This is a web service that helps you securely control access to AWS resources. It can be used in a web scraping project to manage access to AWS services and resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a11b18-2656-45e6-a2d1-3fde1da3402f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2d7bc-977b-41ce-a21c-4fc7222fb4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150cc7d-26dc-4299-a58a-5378f97ea724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16b91ac-1e23-4efd-a3dd-e5eb7a1621fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e943b2-9359-41e2-a9d8-5cd9e466fca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7e86b-7e94-4937-a9cd-21358c15aa56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
